<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Multisensorylab : Lab Website for the Multisensory Lab at Peking University">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Multisensorylab</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
      <!--<a id="forkme_banner" href="https://github.com/hyiltiz/multisensorylab">View on GitHub</a>-->

      <h1 id="project_title">The Multisensory Lab</h1>
      <!--<h2 id="project_tagline">Lab Website for the Multisensory Lab at Peking University</h2>-->

      <!--<section id="downloads">-->
      <!--<a class="zip_download_link" href="https://github.com/hyiltiz/multisensorylab/zipball/master">Download this project as a .zip file</a>-->
      <!--<a class="tar_download_link" href="https://github.com/hyiltiz/multisensorylab/tarball/master">Download this project as a tar.gz file</a>-->
      <!--</section>-->
      </header>
    </div>


    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
      <hr />

      <div style="width: 100%; overflow: hidden;">
        <div style="width: 200px; float: left;"><img style="width: 200px;" src="./images/clh.jpg"/></div>
        <div style="margin-left: 220px;">
          <!--<p>Line 1<p>-->
          <!--<p>Line 2<p>-->
          <!--<p>Line 3<p>-->
          <p><strong>&nbsp;&nbsp;&nbsp;&nbsp;Dr. Lihan Chen</strong><p>
          <ul>
            <li>Room: 1603<br>
            Wang Kezhen Building, 52 HaiDian Road<br>
            HaiDian District, Beijing, China</li>
            <p>
            <li>Department of Psychology, Peking University</li>
            <li>Phone: +86 (0) 10 / 62760057</li>
            <li>Email: CLH@pku.edu.cn</li>
          </ul>
        </div>
      </div>

      <!--OLD SOURCE FOR IMAGE AND INFO-->
      <!--<ul>-->
        <!--<img width=193 height=193 src="./images/clh.jpg" align=left hspace=12 <u><span lang=EN-US><o:p></o:p></span></u>-->
        <!--[><li><p>Adress <img src="./images/clh.jpg" /></p><]-->
        <!--<h4 id="contact-dr.-lihan-chen">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dr. Lihan Chen</h4>-->
        <!--[>&nbsp;&nbsp;<strong>Dr. Lihan Chen</strong><br><]-->
        <!--&nbsp;&nbsp;Room: 1603<br>-->
        <!--&nbsp;&nbsp;Wang Kezhen Building, 52 HaiDian Road<br>-->
        <!--&nbsp;&nbsp;HaiDian District, Beijing, China<br>-->
        <!--&nbsp;&nbsp;Department of Psychology<br>-->
        <!--&nbsp;&nbsp;Peking University<br>-->
        <!--&nbsp;&nbsp;Phone: +86 (0) 10 / 62760057<br>-->
        <!--&nbsp;&nbsp;Email: CLH@pku.edu.cn<br>-->
        <!--<p>&nbsp;</p>-->
        <!--</ul>-->
      <!--OLD SOURCE FOR IMAGE AND INFO-->

      <!--HELLO! edit your content here!-->

<h4 id="research-interests">Research Interests:</h4>
<ul>
<li>Multisensory time perception</li>
<li>Crossmodal attention</li>
<li>Crossmodal correspondence</li>
<li>Tactile perception</li>
</ul>
<h4 id="researchconference-news">Research/Conference News</h4>
<ul>
<li>The research topic, &quot;Sub-and supra-second timing: brain, learning and development&quot; can be found in Frontiers <a href="http://journal.frontiersin.org/ResearchTopic/3525">here</a>.</li>
<li>We are hosting 17<sup>th</sup> International Mutlisensory Research Forum (<a href="http://www.multisensorylab.com/imrf2016/">IMRF 2016</a>).</li>
<li>Associate Editor for Haptic Science, <a href="http://haptics2015.org/">IEEE Word Haptics Conference 2015</a>.</li>
</ul>
<h4 id="current-research-projects">Current Research Projects</h4>
<ul>
<li>NSFC project - Multisensory interaction and perceptual averaging (2013-2015)</li>
<li>863 Program – Audiovisual information processing: Brain-Computer interface, key technology and platform (2012-2015)</li>
</ul>
<h4 id="publications">Publications</h4>
<ul>
<li>Chen L. (2014) How many neural oscillators we need on sub- and supra-second intervals processing in the primate brain. Front. Psychol.5:1263.</li>
<li>Chen L. (2014). Statistical Learning in a Multisensory World. Austin Biom and Biostat.1(1): 3.</li>
<li>Chen, L. &amp; Zhou, X. (2014). Fast transfer of cross-modal time interval training. Experimental Brain Research, 232, 1855-1864.</li>
<li>Chen L., Wang Q., &amp; Bao M (2014). Spatial references and audio-tactile interaction in cross-modal dynamic capture. Multisensory Research, 27,55-70.</li>
<li>Wang Q., Bao M. &amp; Chen L. (2014). The role of spatio-temporal and spectral cues in segregating short sound events: evidence from auditoryTernus display. Experimental Brain Research, 232,273-282.</li>
<li>Chen, L. &amp; Vroomen, J. (2013). Intersensory Binding across Space and Time: A Tutorial Review. Attention,Perception, &amp; Psychophysics,75,790-811. <a href="./media/2013-ChenVroomen-APP.pdf">pdf</a></li>
<li>Chen, L. (2013). Tactile Flash Lag Effect: Taps with Changing Intensities Lead Briefly Flashed Taps. IEEE World Haptics Conference2013, The 5th Joint Eurohaptics Conference and IEEE Haptics Symposium, pp 253-258.</li>
<li>Jiang, Y.,&amp; Chen, L. (2013). Mutual influences of intermodal visual/tactile apparent motion and auditory motion with uncrossed andcrossed arms. Multisensory Research，26,19-51.</li>
<li>Chen, L.(2013). Synaesthetic correspondence between auditory clips and colors: an empirical study.In J. Yang, F. Fang, and C. Sun (Eds): IScIDE2012, Lecture Notes in Computer Science (LNCS) 7751, pp. 507-513,Springer-Verlag Berlin Heidelberg.</li>
<li>Zhang, H., Chen L &amp; Zhou, X. (2012) Adaptation to visual or auditory time intervals modulates the perception of visual apparent motion.Frontiers in Integrative Neuroscience. 6:100.</li>
<li>Chen, L., &amp; Zhou, X. (2011). Capture of intermodal visual/tactile apparent motion by moving and static sounds. Seeing and Perceiving, 24,369-389.</li>
<li>Chen, L., &amp; Zhou, X. (2011). Visual apparent motion can be modulated by task-irrelevant lexical information. Attention, Perception, &amp;Psychophysics, 73, 1010-1015.</li>
<li>Chen L, Shi Z, &amp; Müller H. J. (2011) Interaction of Perceptual Grouping and Crossmodal Temporal Capture in Tactile Apparent-Motion. PLoS ONE6(2): e17130.</li>
<li>Shi, Z., Zou, H., Rank, M., Chen, L., Hirche, S., Müller, H. J. (2010). Effects of packet loss and latency on the temporal discrimination ofvisual-haptic events. IEEE Transactions on Haptics，3 (1)，28-36.</li>
<li>Shi, Z., Chen, L., &amp; Müller , H. J. (2010). Auditory temporal modulation of the visual Ternus effect: the influence of time interval.Experimental Brain Research, 203 (4), 723-735.</li>
<li>Chen, L., Shi, Z., &amp; Müller , H. J. (2010). Influences of intra- and crossmodal grouping on visual and tactile Ternus apparent motion. BrainResearch,1354,152-162.</li>
</ul>
<h4 id="teachings">Teachings</h4>
<ul>
<li>Research methodology in Psychology-Matlab 2011Fall, 2012 Spring, 2013 Spring, 2014 Spring.</li>
<li>Seminar on Multisensory Processing 2013 Spring, 2014 Spring.</li>
</ul>
<h4 id="conference-abstracts">Conference Abstracts</h4>
<ol style="list-style-type: decimal">
<li>Chen, L., Ai, F., Meng, X., Xie, W. (2014) Synaesthetic congruency modulates temporal ventriloquism effect in dyslexia. 15th International Multisensory Research Forum, The Netherlands, Amsterdam.</li>
<li>Chen, L., Wang, X, Yao, L.,&amp; Zhou, X. (2013). Cognitive style predicts the perception of visual apparent motion, Vision Science Society, USA, Naples, Florida.</li>
<li>Yiltiz, H. &amp; Chen, L. (2013) Tactile inputs resolve the ambiguous perception of biological point light walkers, Vision Science Society, USA, Naples, Florida.</li>
<li>Chen, L,&amp; Zhou, X.(2013). Fast transfer of cross-modal interval training needs attentional engagement. 14th International Multisensory Research Forum, Israel, Jerusalem.</li>
<li>Chen, L., Zhang,H.,&amp; Zhou, X.(2012). Adaptation to temporal interval modulates the perception of visual apparent motion, Vision Science Society, USA, Naples, Florida.</li>
<li>Zhou, X., Chen, L.,&amp; Chen,X.(2011). Irregular sound rhythm magnifies the temporal sequential effect in audiovisual temporal ventriloquism. Vision Science Society, USA, Naples, Florida.</li>
<li>Zhang, H., Chen, L. , &amp; Zhou, X.(2011). Selective adaptation for temporal information: Evidence from the classification of visual Ternus apparent motion. 12th International Multisensory Research Forum, Japan, Fukuoka.</li>
<li>Chen, L.,&amp; Zhou, X.(2010). Audiovisual syensthetic correspondence modulates visual apparent motion，11th International Multisensory Research Forum, UK, Liverpool.</li>
<li>Chen, L., Shi, Z.,&amp; Müller, H. J. (2009). Asymmetric crossmodal interaction and priming effects in Ternus illusion. ECVP 2009, Regensburg, Germany, Perception, 38 European Conference on Visual Perception (ECVP) Abstract Supplement, page, 142.</li>
<li>Zou H, Shi Z, Chen L, Rank M, Hirche S, &amp; Müller H (2009). Effects of visual signal dropout and latency on temporal discrimination of visual - haptic collision. Perception, 38 ECVP Abstract Supplement, page 143.</li>
<li>Shi, Z., Chen, L., &amp; Müller, H. J. (2009). Auditory capture on the visual Ternus effect: the influence of subjective inter-sound interval, 10th International Multisensory Research Forum, New York.</li>
<li>Chen, L., Shi, Z., &amp; Müller, H. J. (2008). Auditory modulation of visual Ternus effect. Second International Symposium on Visual Search and Selective Attention, Murten, Switzerland.</li>
<li>Chen, L. Shi. Z., Müller, H. J. , &amp; Zhang, Z. (2008). Temporal cross modal capture of audition and tactile apparent motion, 29th International Congress of Psychology, Berlin.</li>
</ol>

      <!--HELLO! edit your content here!-->
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
      <p class="copyright">Multisensorylab website maintained by <a href="https://github.com/hyiltiz">hyiltiz</a></p>
      <p>Department of Psychology, Peking University</a></p>
    </footer>
  </div>

  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
      try {
        var pageTracker = _gat._getTracker("UA-37620573-7");
        pageTracker._trackPageview();
      } catch(err) {}
</script>


  </body>
</html>
